{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/on-the-construction-of-the-self-lesswrong","result":{"data":{"post":{"slug":"/on-the-construction-of-the-self-lesswrong","title":"On the construction of the self - LessWrong","date":"09.06.2022","tags":null,"description":null,"canonicalUrl":null,"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"On the construction of the self - LessWrong\",\n  \"date\": \"2022-06-09T00:00:00.000Z\",\n  \"slug\": \"/on-the-construction-of-the-self-lesswrong\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lesswrong.com/posts/h2xgbYBNP4dLharg4/on-the-construction-of-the-self\"\n  }, \"https://lesswrong.com/posts/h2xgbYBNP4dLharg4/on-the-construction-of-the-self\")), mdx(\"hr\", null), mdx(\"p\", null, \"Viewed from a multi-agent frame, there is no such thing as \\u201Cnot making decisions\\u201D. Different subsystems are constantly transmitting different kinds of intentions - about what to say, what to do, and even what to think - into consciousness. Even things which do not subjectively feel like decisions, like just sitting still and waiting, are still the outcomes of a constant decision-making process. It is just that a narrative subsystem tags some outcomes as being the result of \\u201Cyou\\u201D having made a decision, while tagging others as things that \\u201Cjust happened\\u201D.\"), mdx(\"p\", null, \"For example, if an unconscious evidence-weighting system decides to bring a daydream into consciousness, causing your mind to wander into the daydream, you may feel that you \\u201Cjust got lost in thought\\u201D. But when a thought about making tea comes into consciousness and you then get up to make tea, you may feel like you \\u201Cdecided to make some tea\\u201D. Ultimately, you have a schema which classifies everything as either \\u201Cdoing\\u201D or \\u201Cnot doing\\u201D, and attempts to place all experience into one of those two categories.\"), mdx(\"hr\", null), mdx(\"p\", null, \"Do not exert any conscious intention to control your mind, such as by preventing yourself from thinking particular thoughts.\\nBut if you notice a sensation of yourself trying to control your mind, also do not do anything to stop yourself from feeling that sensation.\\nOne thing (of many) that may happen when you try to follow these instructions is:\"), mdx(\"p\", null, \"You were told not to have any conscious intention to control your mind, so the subsystems that caused you to sit down and start meditating, take no particular action to shift the contents of consciousness.\\nAt some point, other subsystems, driven by a priority other than meditating, send something into consciousness. They are attempting to change its contents towards the particular priority that these subsystems care about.\\nThis is noticed by the subsystem classifying mental contents as doing or non-doing. It classifies this as \\u201Cdoing\\u201D, and sends a sensation of intentional \\u201Cdoing\\u201D into consciousness.\\nThe subsystem which originally sat down with the intention of meditating and not exerting conscious control, notices the sensation of conscious doing.\\nThe overall mind-system gets confused, because its internal narrative (modeled from the assumption of a unitary self) now says \\u201CI sat down and intended not to intentionally control my mind, but now I find myself intentionally controlling my mind anyway, but how can I unintentionally end up doing something intentional?\\u201D\\nFollowing the original instructions, the non-meditating subsystem may now just let the sensation of conscious doing be there, without interfering with it.\\nEventually, there may be an experiential realization that the sensation of intentional doing is just a sensation or a tag assigned to some particular actions, not intrinsically associated with any single subsystem.\"), mdx(\"hr\", null), mdx(\"p\", null, \"So, translated into multiagent language: \\u201Cdon\\u2019t do anything but also don\\u2019t not do anything\\u201D means \\u201Cyou, the subsystem which is following these instructions: don\\u2019t do anything in particular, but when an intention to do something arises from another subsystem, also don\\u2019t do anything to counteract that intention\\u201D.\"), mdx(\"p\", null, \"Inevitably, you will experience yourself as doing something anyway, because another subsystem has swapped in and out of control, and this registers to you as 'you' having done something. This is an opportunity to notice on an experiential level that you are actually identifying with multiple distinct processes at the same time.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  })));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"https://lesswrong.com/posts/h2xgbYBNP4dLharg4/on-the-construction-of-the-self Viewed from a multi-agent frame, there is no such thing asâ€¦","timeToRead":2,"banner":null}},"pageContext":{"slug":"/on-the-construction-of-the-self-lesswrong","formatString":"DD.MM.YYYY"}},"staticQueryHashes":["2744905544","3090400250","318001574"]}